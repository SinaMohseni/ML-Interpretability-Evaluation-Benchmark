# ML-Interpretability-Evaluation-Benchmark

This repository is a benchmark to evaluate machine learning local explanaitons' quality (explianations from any explainer) for text and image classfiers.

## Main Idea

In order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making, people need to be able to understand the machine rationale for given output.
Research in explain artificial intelligence (XAI) addresses the aim, but there is a need for **evaluation of human relevance**.

### Human Explanation Format
We present an evaluation benchmark for instance explanations from text and image classifiers. 
The explanation meta-data in this benchmark is generated from user annotations (10 coder for each image, 2 for each text article) of image and text samples. 
Our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models.
We use a weighted masks (**_soft-explanations_**) for image samples to describe users opinion on objects in the image. 
Here are some image examples:

![dog 1](Image/user_heatmap/dog3.png) ![dog 2](Image/user_heatmap/dog2.png)  ![car](Image/user_heatmap/car4.png)

Similar, in text articles each word is weighted with two graders to demonstrade explanation weight. Text explanations are listed in JSON format files.


### Use case
This benchmark is designed to qualify local machine-learning explanations with human-grounded explanations. 
One way to compare machine generated explanations (e.g. [Lime](https://github.com/marcotcr/lime) and [RRR](https://github.com/dtak/rrr)) with our benchmark is to user [Precision-Recall](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) metric.

## Benchmark Sctructure
- `Image`
   - `org_img`: Original images which are 100 sample images from 25 different subjects (ImageNet dataset).
   - `user_weighted_mask`: Human grounded explanations for images in form of a weighted mask.
   - `user_heatmap`: A heatmap visualization of user weighted masks.
 Â  - `LIME_overlay`: Explanations generated by LIME from Google inception v3 for image classification.
   
- `Text`
  - `org_documents`: Original documents which are 100 samples from electronic and medical topics (20 newsgroup dataset).
  - `user_evaluation`: Human grounded explanation for text articles in form of weighted words.
  
  
## Citation

Description and details in this paper: http://people.tamu.edu/~sina.mohseni/webpage/Benchmark.pdf

```
@article{mohseni2018human,
  title={A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning},
  author={Mohseni, Sina and Ragan, Eric D},
  journal={arXiv preprint arXiv:1801.05075},
  year={2018}
}
```

More samples and full paper to be uploaded soon.
